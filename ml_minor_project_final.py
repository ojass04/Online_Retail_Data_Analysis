# -*- coding: utf-8 -*-
"""ML_minor_project_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OGI07g9ZbFQzQtR5ExOFCxuw7oGak8ON

#Minor Project

#Importing Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

'''
Importing necessary modules
'''
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import random
import warnings
warnings.filterwarnings('ignore')

'''
Importing dataset from the drive
'''
data = pd.read_csv('/content/drive/MyDrive/Minor_project_ML/Online Retail.csv')
data.head()

data.info()

'''
Finding no of null values in every feature or column
'''
print("No of null values in every columns-")
data.isnull().sum()

"""#Data Preprocessing"""

'''
Deep Copying dataset to preserve original dataset
'''
copy_df = data.copy(deep = True)
copy_df

'''
Dropping those rows where Description is null
'''
copy_df["Description"].replace(np.nan,"null_values",inplace=True)

'''
Converting noisy StockCode to their correct StockCode
'''
for i in range(len(data)):
  if(data["StockCode"].iloc[i][-1]>="A" and data["StockCode"].iloc[i][-1]<="Z"):
    copy_df.at[i,'StockCode'] =  data["StockCode"].iloc[i][:5]+str(ord(data["StockCode"].iloc[i][-1])-ord('A')+1)
  elif(data["StockCode"].iloc[i][-1]>="a" and data["StockCode"].iloc[i][-1]<="z"):
    copy_df.at[i,'StockCode'] =  data["StockCode"].iloc[i][:5]+str(ord(data["StockCode"].iloc[i][-1])-ord('a')+1)
copy_df

'''
Finding no of Stock Codes corresponding to a single Description, will help for further analysis
'''
all_desc = {}
for i in range(len(copy_df)):
  if(copy_df["Description"].iloc[i] in all_desc.keys()):
    all_desc[copy_df["Description"].iloc[i]].add(copy_df["StockCode"].iloc[i])
  else:
    all_desc[copy_df["Description"].iloc[i]]= set([])

print("Description Value with all Stock Codes-")
for i in all_desc:
  if(len(all_desc[i])>1):
    print(i,all_desc[i])

'''
Finding rows which contains flawed Description which can be dropped, by finding if there exist many small case letters and droping
subsequent rows
'''
all_desc_suspicious = list(all_desc.keys())
all_desc_to_drop = []
print("Possible Rows which have suspicious description to drop - ")
for i in all_desc_suspicious:
  temp_str = ""
  i = str(i)
  cnt = 0
  for j in range(len(i)):
    if(i[j].isdigit()==False and i[j]!=" "):
      temp_str += i[j]
    if(i[j]>='a' and i[j]<='z'):
      cnt +=1

  if(temp_str.isupper()==False and cnt>len(i)/2):
    all_desc_to_drop.append(i)
    print(i)

all_sum_drop_values = 0
print("No of values in each description value to drop")
all_desc_with_val_dict = dict(copy_df["Description"].value_counts())
for i in range(len(all_desc_to_drop)):
  print(all_desc_to_drop[i],all_desc_with_val_dict[all_desc_to_drop[i]])
  all_sum_drop_values += all_desc_with_val_dict[all_desc_to_drop[i]]
print("Total values which can be dropped-",all_sum_drop_values)

'''
Droping noisy Description to have final dataset
'''
data_temp = copy_df[copy_df.Description.isin(all_desc_to_drop) == False]
# print(len(data_temp))

# print(len(data_temp))
data_temp = data_temp.reset_index()
data_temp

'''
Extracting Month and Time from dataset and creating corresponding new columns for them
'''
invoice_list = data_temp['InvoiceDate'].to_numpy()
#print(invoice_list)
l = len(invoice_list)
month_list = []
time_list = [] #AM = 0, PM = 1
for i in range(0,l,1):
  temp_list = invoice_list[i].split()
  #print(temp_list)
  temp_str = ''
  for j in range(0,len(temp_list[0]),1):
    if temp_list[0][j] == '/':
      break
    else:
      temp_str += temp_list[0][j]
  month_list.append(int(temp_str))
  temp_str = ''
  for j in range(0,len(temp_list[1]),1):
    if temp_list[1][j] == ':':
      break
    else:
      temp_str += temp_list[1][j]
  temp_str = int(temp_str)
  for k in range(0,6,1):
    if temp_str >= k*4 and temp_str < (k+1)*4:
      time_list.append(k)

  # if temp_str >= 0 and temp_str < 4:
  #   time_list.append(0)
  # elif temp_str >= 4 and temp_str < 8:
  #   time_list.append(1)
  # elif temp_str >= 8 and temp_str < 12:
  #   time_list.append(2)
  # elif temp_str >= 12 and temp_str < 16:
  #   time_list.append(3)
  # elif temp_str >= 16 and temp_str < 20:
  #   time_list.append(4)
  # else:
  #   time_list.append(5)

data_temp['Month'] = month_list
data_temp['Time'] = time_list

data_temp

'''
Converting negative Quantity and UnitPrice to their positive value
'''
for i in range(len(data_temp)):
  if(float(data_temp["UnitPrice"].iloc[i])<0):
    data_temp["UnitPrice"].iloc[i]= -1*float(data_temp["UnitPrice"].iloc[i])
  if(int(data_temp["Quantity"].iloc[i])<0):
    data_temp["Quantity"].iloc[i]= -1*int(data_temp["Quantity"].iloc[i])

data_temp

'''
Finding amount spend by customer on a purchase of a paticular description and thus craeting a new column in dataset
'''
quant_list = data_temp['Quantity'].to_numpy()
price_list = data_temp['UnitPrice'].to_numpy()

l = len(price_list)
amount_list = []

for i in range(0,len(price_list),1):
  temp = quant_list[i]*price_list[i]
  #temp = round(temp,2)
  amount_list.append(temp)

data_temp['Amount'] = amount_list
data_temp

'''
Dropping Invoice Date as month and time has been extracted and new column has been formed
'''
data_temp.drop(['InvoiceDate'],axis = 1,inplace = True)
data_temp

'''
Reindexing dataset after all the transformations and preprocessing
'''

data_temp.drop(['index'],axis = 1,inplace = True)
data_temp

data_temp.drop(["StockCode"],axis=1,inplace=True)

for i in range(len(data_temp)):
  data_temp.at[i,'InvoiceNo']= str(data_temp["InvoiceNo"].iloc[i])

'''
Finally converting every Description to its alpha numeric value , which will further help us remove noisy data
'''
for i in range(len(data_temp)):
  newDesc = ""
  for j in data_temp["Description"].iloc[i]:
    if(j.isalnum()):
      newDesc += j
  data_temp.at[i,"Description"] = newDesc

data_temp

'''
Encoding Country, InvoiceNo and Description
'''

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data_temp['Country'] = label_encoder.fit_transform(data_temp['Country'])

data_temp['InvoiceNo'] = label_encoder.fit_transform(data_temp['InvoiceNo'])
data_temp['Description'] = label_encoder.fit_transform(data_temp['Description'])

data_temp

'''
Obtaining final dataset for model and cluster analysis
'''

data_try = data_temp.dropna(subset = ['CustomerID'])
data_try = data_try.reset_index(drop=True)
data_try

"""#Data Visualization"""

'''
Plotting and visualizing how much money is spent by a single customer ID
'''

dict_customer_amt = {}
for i in range(len(data_temp)):

  if(data_temp["CustomerID"].iloc[i] is not np.nan):
    if(data_temp["CustomerID"].iloc[i] in dict_customer_amt.keys()):
      dict_customer_amt[data_temp["CustomerID"].iloc[i]] += data_temp["Amount"].iloc[i]
    else:
      dict_customer_amt[data_temp["CustomerID"].iloc[i]] = data_temp["Amount"].iloc[i]
plt.scatter(list(dict_customer_amt.keys()),list(dict_customer_amt.values()))
plt.xlabel("Customer ID-")
plt.ylabel("Amount of money spent in a year-")
plt.legend(["Data Point"])
plt.show()

'''
Seeing no of sales of the company in every month of the year
'''

month_names = ["January","February","March","April","May","June","July","August","September","October","November","December"]
updated_names = []
plt.rcdefaults()
plt.figure(figsize=(8,8))
keys_list = list(dict(data_temp["Month"].value_counts()).keys())
for i in range(len(keys_list)):
  updated_names.append(month_names[int(keys_list[i])-1])

explode_values = np.zeros(len(month_names))
explode_values = explode_values+0.1

plt.pie(data_temp["Month"].value_counts(),explode=explode_values)

centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.legend(updated_names)
plt.title("Distribution of Customers wrt Month -")
plt.show()

'''
Seeing how much sale is done in every bin of time
'''
plt.bar([1,2,3,4,5],data_temp["Time"].value_counts(),color="gold")
plt.xlabel("Time->")
plt.ylabel("Count of ocuurence of bill time->")
plt.show()

'''
Finding how much elements were purchased by a person in single visit based on Invoice NO
'''
invoice_no_with_no_of_item = {}
for i in range(len(data_temp)):
  if(data_temp["InvoiceNo"].iloc[i] in invoice_no_with_no_of_item.keys()):
    invoice_no_with_no_of_item[data_temp["InvoiceNo"].iloc[i]] += 1
  else:
    invoice_no_with_no_of_item[data_temp["InvoiceNo"].iloc[i]] = 1

plt.plot(invoice_no_with_no_of_item.values(),color="r")
plt.xlabel("Invoice No->")
plt.ylabel("No of items purchsed->")
plt.title("Distribution showing how many items purchsed in one bill")
plt.show()

'''
Distribution of sales of company according to different countries
'''

plt.rcdefaults()
Country_names = dict(data["Country"].value_counts()).keys()
plt.figure(figsize=(8,10))
explode_values = np.zeros(len(Country_names))
explode_values = 0.35+explode_values

plt.pie(data["Country"].value_counts(),explode=explode_values)
plt.legend(Country_names)
plt.title("Distribution of Customers wrt Country -")
plt.show()

"""#Visualiztion for finding optimum no of clusters-"""

data_try

from sklearn.cluster import KMeans
model = KMeans(n_clusters = 4).fit(data_try)

random_data_x = []
random_data_y = []
no_of_points=50000
for i in range(no_of_points):
  ind= random.randint(0,len(data_try)-1)
  random_data_x.append(data_try.iloc[ind].to_numpy())
  random_data_y.append(model.labels_[ind])
random_data_x = np.array(random_data_x)
data_reduced_for_silhoutte = pd.DataFrame(random_data_x,columns=data_try.columns)

'''
Obtaining and Visualizing InterClusterDistance so that we can see how many clusters are optimum
'''

from yellowbrick.cluster import SilhouetteVisualizer,InterclusterDistance
from yellowbrick.datasets import load_nfl
from yellowbrick.features import RadViz
fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)

model_silhoutte = KMeans(n_clusters=2)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = InterclusterDistance(model_silhoutte, colors='yellowbrick',ax=ax1)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=3)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = InterclusterDistance(model_silhoutte, colors='yellowbrick',ax=ax2)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=4)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = InterclusterDistance(model_silhoutte, colors='yellowbrick',ax=ax3)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=5)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = InterclusterDistance(model_silhoutte, colors='yellowbrick',ax=ax4)
visualizer.fit(data_reduced_for_silhoutte)
visualizer.show()

'''
Obtaining and Visualizing Silhouette Distance and plotting their graph so that we can see how many clusters are optimum
'''
from yellowbrick.cluster import SilhouetteVisualizer,InterclusterDistance
from yellowbrick.datasets import load_nfl
from yellowbrick.features import RadViz
fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)

model_silhoutte = KMeans(n_clusters=2)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = SilhouetteVisualizer(model_silhoutte, colors='yellowbrick',ax=ax1)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=3)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = SilhouetteVisualizer(model_silhoutte, colors='yellowbrick',ax=ax2)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=4)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = SilhouetteVisualizer(model_silhoutte, colors='yellowbrick',ax=ax3)
visualizer.fit(data_reduced_for_silhoutte)
# visualizer.show()
model_silhoutte = KMeans(n_clusters=5)
model_silhoutte.fit(data_reduced_for_silhoutte)
visualizer = SilhouetteVisualizer(model_silhoutte, colors='yellowbrick',ax=ax4)
visualizer.fit(data_reduced_for_silhoutte)
visualizer.show()

'''
Obtaining and Visualizing Inertia of model for various clusters and finally using elbow method so that we can see how many clusters are optimum
'''
from scipy.spatial.distance import cdist
distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
for k in range(2, 8):
    model_temp = KMeans(n_clusters=k).fit(data_try)
    model_temp.fit(data_try)
    distortions.append(sum(np.min(cdist(data_try, model_temp.cluster_centers_,'euclidean'), axis=1)) / data_try.shape[0])
    inertias.append(model_temp.inertia_)
    mapping1[k] = sum(np.min(cdist(data_try, model_temp.cluster_centers_,'euclidean'), axis=1)) / data_try.shape[0]
    mapping2[k] = model_temp.inertia_

plt.plot(range(2,8), inertias, 'bx-',marker="o")
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()

"""#Models

#PCA transformation on dataset
"""

'''
Applying PCA transformation so that to convert 9 dimensional data into 2 dimensional
'''
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

pca.fit(data_try)
pca_df_1 = pca.transform(data_try)

pca_df_1 = pd.DataFrame(data = pca_df_1)
pca_df_1

'''
Converting and transforming dataset into 3 dimension by applying PCA with 3 components
'''

pca_2 = PCA(n_components=3)

pca_2.fit(data_try)
pca_df_2 = pca_2.transform(data_try)

pca_df_2 = pd.DataFrame(data = pca_df_2)
pca_df_2

"""#KMeans Model on PCA with no_of_components=2 and 3"""

'''
Applying KMeans model for both PCA no_of_components = 2 and 3 and consequently plotting their clusters on 2d and 3d plots
'''
from sklearn.cluster import KMeans

model_2 = KMeans(n_clusters = 4)
model_2.fit(pca_df_1)

sse_2 = model_2.inertia_

model_3 = KMeans(n_clusters = 4)
model_3.fit(pca_df_2)

sse_3 = model_3.inertia_

label_2_all = model_2.labels_
label_3_all = model_3.labels_

centroid_2 = model_2.cluster_centers_
centroid_3 = model_3.cluster_centers_

print(centroid_2)
print(centroid_3)

import random

num = 40000
index_list = []
for i in range(0,num,1):
  temp = random.randint(0,len(pca_df_1)-1)
  index_list.append(temp)

index_list = list(set(index_list))
index_list.sort()

print(index_list)

temp_df_1 = pca_df_1.iloc[index_list,:]
temp_df_2 = pca_df_2.iloc[index_list,:]

label_temp = pd.DataFrame(data = label_2_all,columns = ['Y'])
label_2 = label_temp.iloc[index_list,0]
label_2.to_numpy()

label_temp = pd.DataFrame(data = label_3_all,columns = ['Y'])
label_3 = label_temp.iloc[index_list,0]
label_3.to_numpy()

import matplotlib.pyplot as plt
import numpy as np

x1 = temp_df_1.iloc[:,0].to_numpy()
y1 = temp_df_1.iloc[:,1].to_numpy()
plt.figure(figsize=(6,4))

plt.scatter(x1,y1, c = label_2,s=2,cmap = 'viridis')
plt.xlabel("PC1")
plt.ylabel("PC2")

plt.title("Scatter plot for n_comp = 2")

plt.scatter(centroid_2.T[0],centroid_2.T[1],color = 'red', marker = 'o')

plt.show()

x2 = temp_df_2.iloc[:,0].to_numpy()
y2 = temp_df_2.iloc[:,1].to_numpy()
z2 = temp_df_2.iloc[:,2].to_numpy()

fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")

ax.scatter3D(x2, y2, z2, c = label_3,s=1, label = 'Data Points',cmap = 'viridis')
ax.scatter3D(centroid_3.T[0],centroid_3.T[1],centroid_3.T[2],color = 'red', marker = 'o',s = 80,alpha = 1,label = "Centroid")

plt.title("Scatter plot for n_comp = 3")
plt.legend()

plt.show()

"""#Agglomerative Model on PCA with no_of_components=2 and 3"""

'''
Applying Agglomerative or Heirarchial model for both PCA no_of_components = 2 and 3 and consequently plotting their clusters on 2d and 3d plots
'''
agg_df_1 = pca_df_1.iloc[index_list,:]
agg_np_1 = agg_df_1.to_numpy()

agg_df_2 = pca_df_2.iloc[index_list,:]
agg_np_2 = agg_df_2.to_numpy()

from sklearn.cluster import AgglomerativeClustering

agg_1 = AgglomerativeClustering(n_clusters = 4)

agg_1.fit(agg_np_1)

agg_label_1 = agg_1.labels_

from sklearn.cluster import AgglomerativeClustering

agg_2 = AgglomerativeClustering(n_clusters = 4)

agg_2.fit(agg_np_2)

agg_label_2 = agg_2.labels_

from sklearn.neighbors import NearestCentroid

nc = NearestCentroid()

nc.fit(agg_np_1,agg_label_1)
agg_cent_1 = nc.centroids_

nc_2 = NearestCentroid()

nc_2.fit(agg_np_2,agg_label_2)
agg_cent_2 = nc_2.centroids_

print(agg_cent_1.T)
print(agg_cent_2.T)

x1 = (agg_np_1.T)[0]
y1 = (agg_np_1.T)[1]
plt.figure(figsize=(6,4))

plt.scatter(x1,y1, c = agg_label_1,s=2,cmap = 'viridis')
plt.scatter((agg_cent_1.T)[0],(agg_cent_1.T)[1],color = 'red', marker = 'o',alpha = 1)
plt.xlabel("PC1")
plt.ylabel("PC2")

plt.title("Scatter plot for n_comp = 2")

#plt.scatter(centroid_2.T[0],centroid_2.T[1],color = 'red', marker = 'o')

plt.show()

x2 = agg_np_2.T[0]
y2 = agg_np_2.T[1]
z2 = agg_np_2.T[2]

fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")

ax.scatter3D(x2, y2, z2, c = label_3,s=1, label = 'Data Points',cmap = 'viridis')
ax.scatter3D(agg_cent_2.T[0],agg_cent_2.T[1],agg_cent_2.T[2],color = 'red', marker = 'o',s = 80,alpha = 1,label = "Centroid")

plt.title("Scatter plot for n_comp = 3")
plt.legend()

plt.show()

"""#Models on Whole Dataset"""

'''
Applying KMeans model and correspondingly Agglorematively or Heirarchial on whole dataset and consequently plotting their clusters on 2d and 3d plots
'''
model_whole = KMeans(n_clusters = 4)

model_whole.fit(data_try)
whole_labels_all = model_whole.labels_

label_temp = pd.DataFrame(data = whole_labels_all,columns = ['Y'])
whole_labels = label_temp.iloc[index_list,0]
whole_labels.to_numpy()

temp_df_3 = data_try.iloc[index_list,:]

whole_centroids = model_whole.cluster_centers_

col_chosen = 0
col_chosen_2 = 1
x1 = temp_df_3.iloc[:,col_chosen].to_numpy()
y1 = temp_df_3.iloc[:,col_chosen_2].to_numpy()
plt.figure(figsize=(6,4))

plt.scatter(x1,y1, c = whole_labels,s=1,cmap = 'viridis')
plt.xlabel(temp_df_3.columns[col_chosen])
plt.ylabel(temp_df_3.columns[col_chosen_2])

plt.title("Scatter plot for all columns intact")

plt.scatter(whole_centroids.T[col_chosen],whole_centroids.T[col_chosen_2],color = 'red', marker = 'o')

plt.show()

x2 = temp_df_3.iloc[:,0].to_numpy()
y2 = temp_df_3.iloc[:,1].to_numpy()
z2 = temp_df_3.iloc[:,4].to_numpy()

fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")

ax.scatter3D(x2, y2, z2, c = whole_labels,s=1, label = 'Data Points',cmap = 'viridis')
ax.scatter3D(whole_centroids.T[0],whole_centroids.T[1],whole_centroids.T[4],color = 'red', marker = 'o',s = 80,alpha = 1,label = "Centroid")

plt.title("Scatter plot for all columns intact")
plt.legend()

plt.show()

agg_df_3 = data_try.iloc[index_list,:]
agg_np_3 = agg_df_3.to_numpy()

from sklearn.cluster import AgglomerativeClustering

agg_3 = AgglomerativeClustering(n_clusters = 4)

agg_3.fit(agg_np_3)

agg_label_3 = agg_3.labels_

nc_3 = NearestCentroid()

nc_3.fit(agg_np_3,agg_label_3)
agg_cent_3 = nc_3.centroids_

x1 = (agg_np_3.T)[0]
y1 = (agg_np_3.T)[1]
plt.figure(figsize=(6,4))

plt.scatter(x1,y1, c = agg_label_3,s=2,cmap = 'viridis')
plt.scatter((agg_cent_3.T)[0],(agg_cent_3.T)[1],color = 'red', marker = 'o',alpha = 1)
plt.xlabel("PC1")
plt.ylabel("PC2")

plt.title("Scatter plot for all columns intact")

#plt.scatter(centroid_2.T[0],centroid_2.T[1],color = 'red', marker = 'o')

plt.show()

x2 = agg_np_3.T[0]
y2 = agg_np_3.T[1]
z2 = agg_np_3.T[4]

fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")

ax.scatter3D(x2, y2, z2, c = agg_label_3,s=1, label = 'Data Points',cmap = 'viridis')
ax.scatter3D(agg_cent_3.T[0],agg_cent_3.T[1],agg_cent_3.T[4],color = 'red', marker = 'o',s = 80,alpha = 1,label = "Centroid")

plt.title("Scatter plot for all columns intact")
plt.legend()

plt.show()

whole_centroids

'''
Plotting and Visualizing clusters on all subsets by taking two columsn from all columns
'''
label_list = whole_labels_all
for l in range(len(data_try.columns)):
  for k in range(l+1,len(data_try.columns)):
    if(l!=k):
      ind = []
      for i in range(5000):
        ind.append(random.randint(0,len(data_try)-1))
      y_ind = []
      for i in ind:
        y_ind.append(label_list[i])

      plt.scatter(data_try[data_try.columns[l]].iloc[ind].to_numpy(),data_try[data_try.columns[k]].iloc[ind].to_numpy(),c = y_ind,cmap="viridis")
      for j in range(4):
        plt.scatter(whole_centroids[j][l],whole_centroids[j][k],color="r")
      plt.xlabel(data_try.columns[l])
      plt.ylabel(data_try.columns[k])

      plt.show()

"""#Results"""

'''
These are all the metrics which we used for finally seeing how are model performed on all the PCA with no_of_components=2,3 and further
on whole dataset
'''
from sklearn.metrics import silhouette_score,calinski_harabasz_score,davies_bouldin_score

def cal_metrics(dataset,isKmeans = True):
  if isKmeans == True:
    model = KMeans(n_clusters = 4)
  else:
    model = AgglomerativeClustering(n_clusters = 4)

  model.fit(dataset)
  if isKmeans == True:
    inertia = model.inertia_
  else:
    inertia = "-"
  sil_score = silhouette_score(dataset, model.labels_)
  chs = calinski_harabasz_score(dataset, model.labels_)
  dbs = davies_bouldin_score(dataset, model.labels_)

  return inertia,sil_score,chs,dbs

metric_list = []
df_list = [agg_np_1,agg_np_2,agg_np_3]

for i in range(0,6,1):
  if i<3:
    temp_metrics = cal_metrics(df_list[i])
  else:
    temp_metrics = cal_metrics(df_list[i-3],False)
  metric_list.append(temp_metrics)

print(metric_list)

'''
Finally printing and craeting dataframe for all the scores of all models with all datasets both original and PCA transformed.
'''
metric_list = np.array(metric_list)
metric_df = pd.DataFrame(index=['PCA n_comp = 2','PCA n_comp = 3','Original Dataset','PCA n_comp = 2','PCA n_comp = 3','Original Dataset'])
# metric_df['Dataframe'] =
metric_df['K-Means'] = ['Yes','Yes','Yes','No','No','No']
metric_df['Hierarchical'] = ['No','No','No','Yes','Yes','Yes']
metric_df['Inertia'] = metric_list.T[0]
metric_df['Silhouette Score'] = metric_list.T[1]
metric_df['Calinski Harabasz score'] = metric_list.T[2]
metric_df['Davies Bouldin score'] = metric_list.T[3]

metric_df